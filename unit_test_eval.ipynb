{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sreekarm/SWE-bench/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-04 16:03:19,972 - datasets - INFO - PyTorch version 2.4.0 available.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import hashlib\n",
    "import json\n",
    "import platform\n",
    "import re\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Union, cast\n",
    "\n",
    "from swebench.harness.constants import NON_TEST_EXTS\n",
    "\n",
    "from swebench.harness.constants import (\n",
    "    SWEbenchInstance,\n",
    "    KEY_INSTANCE_ID,\n",
    "    FAIL_TO_PASS,\n",
    "    PASS_TO_PASS,\n",
    "    MAP_REPO_TO_INSTALL,\n",
    "    MAP_REPO_VERSION_TO_SPECS,\n",
    "    USE_X86,\n",
    ")\n",
    "from swebench.harness.dockerfiles import (\n",
    "    get_dockerfile_base,\n",
    "    get_dockerfile_env,\n",
    "    get_dockerfile_instance,\n",
    ")\n",
    "from swebench.harness.utils import (\n",
    "    get_requirements,\n",
    "    get_environment_yml,\n",
    "    get_test_directives,\n",
    ")\n",
    "\n",
    "# Meta imports?\n",
    "from swebench.harness.run_evaluation import *\n",
    "from swebench.harness.test_spec import make_repo_script_list, make_env_script_list\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "splits = {\n",
    "    \"dev\": \"data/dev-00000-of-00001.parquet\",\n",
    "    \"test\": \"data/test-00000-of-00001.parquet\",\n",
    "}\n",
    "df = pd.read_parquet(\"hf://datasets/princeton-nlp/SWE-bench_Lite/\" + splits[\"dev\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ut_make_eval_script_list(\n",
    "    instance, specs, env_name, repo_directory, base_commit, test_patch\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies the test patch and runs the tests.\n",
    "    \"\"\"\n",
    "    HEREDOC_DELIMITER = \"EOF_114329324912\"\n",
    "    DIFF_MODIFIED_FILE_REGEX = r\"--- a/(.*)\"\n",
    "    test_files = re.findall(DIFF_MODIFIED_FILE_REGEX, test_patch)\n",
    "    # Reset test files to the state they should be in before the patch.\n",
    "    reset_tests_command = f\"git checkout {base_commit} {' '.join(test_files)}\"\n",
    "    apply_test_patch_command = (\n",
    "        f\"git apply -v - <<'{HEREDOC_DELIMITER}'\\n{test_patch}\\n{HEREDOC_DELIMITER}\"\n",
    "    )\n",
    "\n",
    "    ############################\n",
    "    file_pat = r\"diff --git a/.* b/(.*)\"\n",
    "    test_pat = r\"\\+class (.*):\"\n",
    "    file_directives = re.findall(file_pat, test_patch)\n",
    "    test_directives = re.findall(test_pat, test_patch)\n",
    "\n",
    "    test_command = \"pytest \"\n",
    "    for file in file_directives:\n",
    "        for test in test_directives:\n",
    "            test_command += f\"{file}::{test} \"\n",
    "    test_command = test_command.strip()\n",
    "\n",
    "    ############################\n",
    "\n",
    "    eval_commands = [\n",
    "        f\"source /opt/miniconda3/bin/activate\",\n",
    "        f\"conda activate {env_name}\",\n",
    "        f\"cd {repo_directory}\",\n",
    "    ]\n",
    "    if \"eval_commands\" in specs:\n",
    "        eval_commands += specs[\"eval_commands\"]\n",
    "    eval_commands += [\n",
    "        f\"git config --global --add safe.directory {repo_directory}\",  # for nonroot user\n",
    "        f\"cd {repo_directory}\",\n",
    "        # This is just informational, so we have a record\n",
    "        f\"git status\",\n",
    "        f\"git show\",\n",
    "        f\"git diff {base_commit}\",\n",
    "        \"source /opt/miniconda3/bin/activate\",\n",
    "        f\"conda activate {env_name}\",\n",
    "    ]\n",
    "    if \"install\" in specs:\n",
    "        eval_commands.append(specs[\"install\"])\n",
    "    eval_commands += [\n",
    "        reset_tests_command,\n",
    "        apply_test_patch_command,\n",
    "        test_command,\n",
    "        reset_tests_command,  # Revert tests after done, leave the repo in the same state as before\n",
    "    ]\n",
    "    return eval_commands\n",
    "\n",
    "\n",
    "def ut_make_test_spec(instance: SWEbenchInstance) -> TestSpec:\n",
    "    if isinstance(instance, TestSpec):\n",
    "        return instance\n",
    "    instance_id = instance[KEY_INSTANCE_ID]\n",
    "    repo = instance[\"repo\"]\n",
    "    version = instance[\"version\"]\n",
    "    base_commit = instance[\"base_commit\"]\n",
    "    problem_statement = instance[\"problem_statement\"]\n",
    "    hints_text = instance[\"hints_text\"]  # Unused\n",
    "    test_patch = instance[\"test_patch\"]\n",
    "\n",
    "    def _from_json_or_obj(key: str) -> Any:\n",
    "        \"\"\"If key points to string, load with json\"\"\"\n",
    "        if isinstance(instance[key], str):\n",
    "            return json.loads(instance[key])\n",
    "        return instance[key]\n",
    "\n",
    "    pass_to_pass = _from_json_or_obj(PASS_TO_PASS)\n",
    "    fail_to_pass = _from_json_or_obj(FAIL_TO_PASS)\n",
    "\n",
    "    env_name = \"testbed\"\n",
    "    repo_directory = f\"/{env_name}\"\n",
    "    specs = MAP_REPO_VERSION_TO_SPECS[repo][version]\n",
    "\n",
    "    repo_script_list = make_repo_script_list(\n",
    "        specs, repo, repo_directory, base_commit, env_name\n",
    "    )\n",
    "    env_script_list = make_env_script_list(instance, specs, env_name)\n",
    "    eval_script_list = ut_make_eval_script_list(\n",
    "        instance, specs, env_name, repo_directory, base_commit, test_patch\n",
    "    )\n",
    "    if platform.machine() in {\"aarch64\", \"arm64\"}:\n",
    "        # use arm64 unless explicitly specified\n",
    "        arch = \"arm64\" if instance_id not in USE_X86 else \"x86_64\"\n",
    "    else:\n",
    "        arch = \"x86_64\"\n",
    "\n",
    "    return TestSpec(\n",
    "        instance_id=instance_id,\n",
    "        repo=repo,\n",
    "        env_script_list=env_script_list,\n",
    "        repo_script_list=repo_script_list,\n",
    "        eval_script_list=eval_script_list,\n",
    "        version=version,\n",
    "        arch=arch,\n",
    "        FAIL_TO_PASS=fail_to_pass,\n",
    "        PASS_TO_PASS=pass_to_pass,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ut_run_instance(\n",
    "    test_spec: TestSpec,\n",
    "    pred: dict,\n",
    "    rm_image: bool,\n",
    "    force_rebuild: bool,\n",
    "    client: docker.DockerClient,\n",
    "    run_id: str,\n",
    "    timeout: int | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a single instance with the given prediction.\n",
    "\n",
    "    Args:\n",
    "        test_spec (TestSpec): TestSpec instance\n",
    "        pred (dict): Prediction w/ model_name_or_path, model_patch, instance_id\n",
    "        rm_image (bool): Whether to remove the image after running\n",
    "        force_rebuild (bool): Whether to force rebuild the image\n",
    "        client (docker.DockerClient): Docker client\n",
    "        run_id (str): Run ID\n",
    "        timeout (int): Timeout for running tests\n",
    "    \"\"\"\n",
    "    # Set up logging directory\n",
    "    instance_id = test_spec.instance_id\n",
    "    model_name_or_path = pred.get(\"model_name_or_path\", \"None\").replace(\"/\", \"__\")\n",
    "    log_dir = RUN_EVALUATION_LOG_DIR / run_id / model_name_or_path / instance_id\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Link the image build dir in the log dir\n",
    "    build_dir = INSTANCE_IMAGE_BUILD_DIR / test_spec.instance_image_key.replace(\n",
    "        \":\", \"__\"\n",
    "    )\n",
    "    image_build_link = log_dir / \"image_build_dir\"\n",
    "    if not image_build_link.exists():\n",
    "        try:\n",
    "            # link the image build dir in the log dir\n",
    "            image_build_link.symlink_to(build_dir.absolute(), target_is_directory=True)\n",
    "        except:\n",
    "            # some error, idk why\n",
    "            pass\n",
    "    log_file = log_dir / \"run_instance.log\"\n",
    "\n",
    "    # Set up report file + logger\n",
    "    report_path = log_dir / \"report.json\"\n",
    "    if report_path.exists():\n",
    "        return instance_id, json.loads(report_path.read_text())\n",
    "    logger = setup_logger(instance_id, log_file)\n",
    "\n",
    "    # Run the instance\n",
    "    container = None\n",
    "    try:\n",
    "        # Build + start instance container (instance image should already be built)\n",
    "        container = build_container(\n",
    "            test_spec, client, run_id, logger, rm_image, force_rebuild\n",
    "        )\n",
    "        container.start()\n",
    "        logger.info(f\"Container for {instance_id} started: {container.id}\")\n",
    "\n",
    "        ###########################################################\n",
    "        # Run unit test eval script and write output to logs\n",
    "        eval_file = Path(log_dir / \"pre_fix_eval.sh\")\n",
    "        eval_file.write_text(test_spec.eval_script)\n",
    "        logger.info(\n",
    "            f\"Eval script for {instance_id} written to {eval_file}; copying to container...\"\n",
    "        )\n",
    "        copy_to_container(container, eval_file, Path(\"/eval.sh\"))\n",
    "        test_output, timed_out, total_runtime = exec_run_with_timeout(\n",
    "            container, \"/bin/bash /pre_fix_eval.sh\", timeout\n",
    "        )\n",
    "        test_output_path = log_dir / \"pre_fix_test_output.txt\"\n",
    "        logger.info(f\"Test runtime: {total_runtime:_.2f} seconds\")\n",
    "        with open(test_output_path, \"w\") as f:\n",
    "            f.write(test_output)\n",
    "            logger.info(f\"Test output for {instance_id} written to {test_output_path}\")\n",
    "            if timed_out:\n",
    "                f.write(f\"\\n\\nTimeout error: {timeout} seconds exceeded.\")\n",
    "                raise EvaluationError(\n",
    "                    instance_id,\n",
    "                    f\"Test timed out after {timeout} seconds.\",\n",
    "                    logger,\n",
    "                )\n",
    "\n",
    "        ###########################################################\n",
    "\n",
    "        # Copy model prediction as patch file to container\n",
    "        patch_file = Path(log_dir / \"patch.diff\")\n",
    "        patch_file.write_text(pred[\"model_patch\"] or \"\")\n",
    "        logger.info(\n",
    "            f\"Intermediate patch for {instance_id} written to {patch_file}, now applying to container...\"\n",
    "        )\n",
    "        copy_to_container(container, patch_file, Path(\"/tmp/patch.diff\"))\n",
    "\n",
    "        # Attempt to apply patch to container\n",
    "        val = container.exec_run(\n",
    "            \"git apply --allow-empty -v /tmp/patch.diff\",\n",
    "            workdir=\"/testbed\",\n",
    "            user=\"root\",\n",
    "        )\n",
    "        if val.exit_code != 0:\n",
    "            logger.info(f\"Failed to apply patch to container, trying again...\")\n",
    "\n",
    "            # try \"patch --batch --fuzz=5 -p1 -i {patch_path}\" to try again\n",
    "            val = container.exec_run(\n",
    "                \"patch --batch --fuzz=5 -p1 -i /tmp/patch.diff\",\n",
    "                workdir=\"/testbed\",\n",
    "                user=\"root\",\n",
    "            )\n",
    "            if val.exit_code != 0:\n",
    "                logger.info(f\"{APPLY_PATCH_FAIL}:\\n{val.output.decode('utf-8')}\")\n",
    "                raise EvaluationError(\n",
    "                    instance_id,\n",
    "                    f\"{APPLY_PATCH_FAIL}:\\n{val.output.decode('utf-8')}\",\n",
    "                    logger,\n",
    "                )\n",
    "            else:\n",
    "                logger.info(f\"{APPLY_PATCH_PASS}:\\n{val.output.decode('utf-8')}\")\n",
    "        else:\n",
    "            logger.info(f\"{APPLY_PATCH_PASS}:\\n{val.output.decode('utf-8')}\")\n",
    "\n",
    "        # Get git diff before running eval script\n",
    "        git_diff_output_before = (\n",
    "            container.exec_run(\"git diff\", workdir=\"/testbed\")\n",
    "            .output.decode(\"utf-8\")\n",
    "            .strip()\n",
    "        )\n",
    "        logger.info(f\"Git diff before:\\n{git_diff_output_before}\")\n",
    "\n",
    "        eval_file = Path(log_dir / \"eval.sh\")\n",
    "        eval_file.write_text(test_spec.eval_script)\n",
    "        logger.info(\n",
    "            f\"Eval script for {instance_id} written to {eval_file}; copying to container...\"\n",
    "        )\n",
    "        copy_to_container(container, eval_file, Path(\"/eval.sh\"))\n",
    "\n",
    "        # Run eval script, write output to logs\n",
    "        test_output, timed_out, total_runtime = exec_run_with_timeout(\n",
    "            container, \"/bin/bash /eval.sh\", timeout\n",
    "        )\n",
    "        test_output_path = log_dir / \"test_output.txt\"\n",
    "        logger.info(f\"Test runtime: {total_runtime:_.2f} seconds\")\n",
    "        with open(test_output_path, \"w\") as f:\n",
    "            f.write(test_output)\n",
    "            logger.info(f\"Test output for {instance_id} written to {test_output_path}\")\n",
    "            if timed_out:\n",
    "                f.write(f\"\\n\\nTimeout error: {timeout} seconds exceeded.\")\n",
    "                raise EvaluationError(\n",
    "                    instance_id,\n",
    "                    f\"Test timed out after {timeout} seconds.\",\n",
    "                    logger,\n",
    "                )\n",
    "\n",
    "        # Get git diff after running eval script\n",
    "        git_diff_output_after = (\n",
    "            container.exec_run(\"git diff\", workdir=\"/testbed\")\n",
    "            .output.decode(\"utf-8\")\n",
    "            .strip()\n",
    "        )\n",
    "\n",
    "        # Check if git diff changed after running eval script\n",
    "        logger.info(f\"Git diff after:\\n{git_diff_output_after}\")\n",
    "        if git_diff_output_after != git_diff_output_before:\n",
    "            logger.info(f\"Git diff changed after running eval script\")\n",
    "\n",
    "        # Get report from test output\n",
    "        logger.info(f\"Grading answer for {instance_id}...\")\n",
    "        report = get_eval_report(\n",
    "            test_spec=test_spec,\n",
    "            prediction=pred,\n",
    "            log_path=test_output_path,\n",
    "            include_tests_status=True,\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"report: {report}\\n\"\n",
    "            f\"Result for {instance_id}: resolved: {report[instance_id]['resolved']}\"\n",
    "        )\n",
    "\n",
    "        # Write report to report.json\n",
    "        with open(report_path, \"w\") as f:\n",
    "            f.write(json.dumps(report, indent=4))\n",
    "        return instance_id, report\n",
    "    except EvaluationError as e:\n",
    "        error_msg = traceback.format_exc()\n",
    "        logger.info(error_msg)\n",
    "        print(e)\n",
    "    except BuildImageError as e:\n",
    "        error_msg = traceback.format_exc()\n",
    "        logger.info(error_msg)\n",
    "        print(e)\n",
    "    except Exception as e:\n",
    "        error_msg = (\n",
    "            f\"Error in evaluating model for {instance_id}: {e}\\n\"\n",
    "            f\"{traceback.format_exc()}\\n\"\n",
    "            f\"Check ({logger.log_file}) for more information.\"\n",
    "        )\n",
    "        logger.error(error_msg)\n",
    "    finally:\n",
    "        # Remove instance container + image, close logger\n",
    "        cleanup_container(client, container, logger)\n",
    "        if rm_image:\n",
    "            remove_image(client, test_spec.instance_image_key, logger)\n",
    "        close_logger(logger)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ut_run_instances(\n",
    "    predictions: dict,\n",
    "    instances: list,\n",
    "    cache_level: str,\n",
    "    clean: bool,\n",
    "    force_rebuild: bool,\n",
    "    max_workers: int,\n",
    "    run_id: str,\n",
    "    timeout: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run all instances for the given predictions in parallel.\n",
    "\n",
    "    Args:\n",
    "        predictions (dict): Predictions dict generated by the model\n",
    "        instances (list): List of instances\n",
    "        cache_level (str): Cache level\n",
    "        clean (bool): Clean images above cache level\n",
    "        force_rebuild (bool): Force rebuild images\n",
    "        max_workers (int): Maximum number of workers\n",
    "        run_id (str): Run ID\n",
    "        timeout (int): Timeout for running tests\n",
    "    \"\"\"\n",
    "    client = docker.from_env()\n",
    "    test_specs = list(map(ut_make_test_spec, instances))\n",
    "\n",
    "    # print number of existing instance images\n",
    "    instance_image_ids = {x.instance_image_key for x in test_specs}\n",
    "    existing_images = {\n",
    "        tag\n",
    "        for i in client.images.list(all=True)\n",
    "        for tag in i.tags\n",
    "        if tag in instance_image_ids\n",
    "    }\n",
    "    if not force_rebuild and len(existing_images):\n",
    "        print(\n",
    "            f\"Found {len(existing_images)} existing instance images. Will reuse them.\"\n",
    "        )\n",
    "\n",
    "    # run instances in parallel\n",
    "    print(f\"Running {len(instances)} instances...\")\n",
    "    with tqdm(total=len(instances), smoothing=0) as pbar:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Create a future for running each instance\n",
    "            futures = {\n",
    "                executor.submit(\n",
    "                    ut_run_instance,\n",
    "                    test_spec,\n",
    "                    predictions[test_spec.instance_id],\n",
    "                    should_remove(\n",
    "                        test_spec.instance_image_key,\n",
    "                        cache_level,\n",
    "                        clean,\n",
    "                        existing_images,\n",
    "                    ),\n",
    "                    force_rebuild,\n",
    "                    client,\n",
    "                    run_id,\n",
    "                    timeout,\n",
    "                ): None\n",
    "                for test_spec in test_specs\n",
    "            }\n",
    "            # Wait for each future to complete\n",
    "            for future in as_completed(futures):\n",
    "                pbar.update(1)\n",
    "                try:\n",
    "                    # Update progress bar, check if instance ran successfully\n",
    "                    future.result()\n",
    "                except Exception as e:\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "    print(\"All instances run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ut_main(\n",
    "    dataset_name: str,\n",
    "    split: str,\n",
    "    instance_ids: list,\n",
    "    predictions_path: str,\n",
    "    max_workers: int,\n",
    "    force_rebuild: bool,\n",
    "    cache_level: str,\n",
    "    clean: bool,\n",
    "    open_file_limit: int,\n",
    "    run_id: str,\n",
    "    timeout: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run evaluation harness for the given dataset and predictions.\n",
    "    \"\"\"\n",
    "    # set open file limit\n",
    "    assert len(run_id) > 0, \"Run ID must be provided\"\n",
    "    resource.setrlimit(resource.RLIMIT_NOFILE, (open_file_limit, open_file_limit))\n",
    "    client = docker.from_env()\n",
    "\n",
    "    # load predictions as map of instance_id to prediction\n",
    "    if predictions_path == \"gold\":\n",
    "        print(\"Using gold predictions - ignoring predictions_path\")\n",
    "        predictions = get_gold_predictions(dataset_name, split)\n",
    "    else:\n",
    "        if predictions_path.endswith(\".json\"):\n",
    "            with open(predictions_path, \"r\") as f:\n",
    "                predictions = json.load(f)\n",
    "        elif predictions_path.endswith(\".jsonl\"):\n",
    "            with open(predictions_path, \"r\") as f:\n",
    "                predictions = [json.loads(line) for line in f]\n",
    "        else:\n",
    "            raise ValueError('Predictions path must be \"gold\", .json, or .jsonl')\n",
    "    predictions = {pred[KEY_INSTANCE_ID]: pred for pred in predictions}\n",
    "\n",
    "    # get dataset from predictions\n",
    "    dataset = get_dataset_from_preds(\n",
    "        dataset_name, split, instance_ids, predictions, run_id\n",
    "    )\n",
    "    full_dataset = load_swebench_dataset(dataset_name, split, instance_ids)\n",
    "    existing_images = list_images(client)\n",
    "    print(f\"Running {len(dataset)} unevaluated instances...\")\n",
    "    if not dataset:\n",
    "        print(\"No instances to run.\")\n",
    "    else:\n",
    "        # build environment images + run instances\n",
    "        build_env_images(client, dataset, force_rebuild, max_workers)\n",
    "        ut_run_instances(\n",
    "            predictions,\n",
    "            dataset,\n",
    "            cache_level,\n",
    "            clean,\n",
    "            force_rebuild,\n",
    "            max_workers,\n",
    "            run_id,\n",
    "            timeout,\n",
    "        )\n",
    "\n",
    "    # clean images + make final report\n",
    "    clean_images(client, existing_images, cache_level, clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make pytest cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut_test_patch = \"\"\"diff --git a/tests/test_fields.py b/tests/test_fields.py\n",
    "index b0a5a77..4e9596f 100644\n",
    "--- a/tests/test_fields.py\n",
    "+++ b/tests/test_fields.py\n",
    "@@ -93,7 +93,20 @@ class TestField:\n",
    "         assert result == {\"_NaMe\": \"Monty\"}\n",
    " \n",
    " \n",
    "-class TestParentAndName:\n",
    "+class TestDateTimeFieldInListOrTuple:\n",
    "+    def test_datetime_field_in_list(self):\n",
    "+        class MySchema(Schema):\n",
    "+            times = fields.List(fields.DateTime())\n",
    "+\n",
    "+        with pytest.raises(AttributeError, match=\"'List' object has no attribute 'opts'\"):\n",
    "+            MySchema()\n",
    "+\n",
    "+    def test_datetime_field_in_tuple(self):\n",
    "+        class MySchema(Schema):\n",
    "+            times = fields.Tuple((fields.DateTime(),))\n",
    "+\n",
    "+        with pytest.raises(AttributeError, match=\"'Tuple' object has no attribute 'opts'\"):\n",
    "+            MySchema()\n",
    "     class MySchema(Schema):\n",
    "         foo = fields.Field()\n",
    "         bar = fields.List(fields.Str()))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pat = r\"diff --git a/.* b/(.*)\"\n",
    "class_pat = r\"\\+class (.*):\"\n",
    "file_directives = re.findall(file_pat, ut_test_patch)\n",
    "class_directives = re.findall(class_pat, ut_test_patch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytest_cmd = \"pytest \"\n",
    "for file in file_directives:\n",
    "    for test in class_directives:\n",
    "        pytest_cmd += f\"{file}::{test} \"\n",
    "pytest_cmd = pytest_cmd.strip()\n",
    "\n",
    "pytest_cmd == \"pytest tests/test_fields.py::TestDateTimeFieldInListOrTuple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ut_load_swebench_dataset(\n",
    "    name=\"princeton-nlp/SWE-bench\", split=\"test\", instance_ids=None\n",
    ") -> list[SWEbenchInstance]:\n",
    "    \"\"\"\n",
    "    Load SWE-bench dataset from Hugging Face Datasets or local .json/.jsonl file\n",
    "    \"\"\"\n",
    "    # check that all instance IDs are in the dataset\n",
    "    if instance_ids:\n",
    "        instance_ids = set(instance_ids)\n",
    "    # Load from local .json/.jsonl file\n",
    "    if name.endswith(\".json\") or name.endswith(\".jsonl\"):\n",
    "        dataset = json.loads(Path(name).read_text())\n",
    "        dataset_ids = {instance[KEY_INSTANCE_ID] for instance in dataset}\n",
    "    if instance_ids:\n",
    "        if instance_ids - dataset_ids:\n",
    "            raise ValueError(\n",
    "                (\n",
    "                    \"Some instance IDs not found in dataset!\"\n",
    "                    f\"\\nMissing IDs:\\n{' '.join(instance_ids - dataset_ids)}\"\n",
    "                )\n",
    "            )\n",
    "        dataset = [\n",
    "            instance\n",
    "            for instance in dataset\n",
    "            if instance[KEY_INSTANCE_ID] in instance_ids\n",
    "        ]\n",
    "    return [cast(SWEbenchInstance, instance) for instance in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1 unevaluated instances...\n",
      "Base image sweb.base.arm64:latest already exists, skipping build.\n",
      "Base images built successfully.\n",
      "No environment images need to be built.\n",
      "Running 1 instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:28<00:00, 29.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All instances run.\n",
      "Cleaning cached images...\n",
      "Removed 0 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = ut_load_swebench_dataset(\n",
    "    \"/Users/sreekarm/SWE-bench/swe-bench-lite-dev-dataset.json\", \"dev\"\n",
    ")\n",
    "run_ids = {i[KEY_INSTANCE_ID] for i in dataset}\n",
    "\n",
    "ut_main(\n",
    "    dataset_name=\"/Users/sreekarm/SWE-bench/swe-bench-lite-dev-dataset.json\",\n",
    "    predictions_path=\"/Users/sreekarm/SWE-bench/ut_predictions.json\",\n",
    "    split=\"dev\",\n",
    "    instance_ids=run_ids,\n",
    "    max_workers=1,\n",
    "    force_rebuild=False,\n",
    "    cache_level=\"env\",\n",
    "    clean=False,\n",
    "    open_file_limit=1024,\n",
    "    run_id=\"test\",\n",
    "    timeout=600,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
